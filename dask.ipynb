{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Tacab\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import scale\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = [2500, 2500, 2500, 2500, 2500, 2500]\n",
    "N_test_start = [2500, 2500, 2500, 2500, 2500, 2500]\n",
    "N_test = [250, 250, 250, 250, 250, 250]\n",
    "N_class = len(N_train)\n",
    "data_sizeX = 200\n",
    "data_sizeY = 600\n",
    "mask_sizeX = data_sizeX\n",
    "mask_sizeY = data_sizeY\n",
    "num_channel = 1\n",
    "train_data1 = []\n",
    "train_data2 = []\n",
    "train_mask = []\n",
    "test_data1 = []\n",
    "test_data2 = []\n",
    "test_mask = []\n",
    "path = 'C:/Users/Tacab/Desktop/derin_ogrenme_gpr/' \n",
    "sub_path = 'expa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_class):\n",
    "    for j in range(N_train[i]):\n",
    "        data1 = sio.loadmat(path + 'dataset/%d/data1/%d.mat' % (i + 1, j + 1))['data1']\n",
    "        resized_data1 = np.array(Image.fromarray(data1).resize((data_sizeY, data_sizeX)))\n",
    "        train_data1.append(resized_data1.reshape((data_sizeX, data_sizeY, 1)))\n",
    "\n",
    "        data2 = sio.loadmat(path + 'dataset/%d/data2/%d.mat' % (i + 1, j + 1))['data2']\n",
    "        resized_data2 = np.array(Image.fromarray(data2).resize((data_sizeY, data_sizeX)))\n",
    "        train_data2.append(resized_data2.reshape((data_sizeX, data_sizeY, 1)))\n",
    "\n",
    "        mask = sio.loadmat(path + 'dataset/%d/mask/%d.mat' % (i + 1, j + 1))['mask']\n",
    "        resized_mask = np.array(Image.fromarray(mask).resize((mask_sizeY, mask_sizeX)))\n",
    "        train_mask.append(resized_mask.reshape((mask_sizeX, mask_sizeY, 1)))\n",
    "\n",
    "    for j in range(N_test_start[i], N_test_start[i] + N_test[i]):\n",
    "        data1 = sio.loadmat(path + 'dataset/%d/data1/%d.mat' % (i + 1, j + 1))['data1']\n",
    "        resized_data1 = np.array(Image.fromarray(data1).resize((data_sizeY, data_sizeX)))\n",
    "        test_data1.append(resized_data1.reshape((data_sizeX, data_sizeY, 1)))\n",
    "\n",
    "        data2 = sio.loadmat(path + 'dataset/%d/data2/%d.mat' % (i + 1, j + 1))['data2']\n",
    "        resized_data2 = np.array(Image.fromarray(data2).resize((data_sizeY, data_sizeX)))\n",
    "        test_data2.append(resized_data2.reshape((data_sizeX, data_sizeY, 1)))\n",
    "\n",
    "        mask = sio.loadmat(path + 'dataset/%d/mask/%d.mat' % (i + 1, j + 1))['mask']\n",
    "        resized_mask = np.array(Image.fromarray(mask).resize((mask_sizeY, mask_sizeX)))\n",
    "        test_mask.append(resized_mask.reshape((mask_sizeX, mask_sizeY, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = da.from_array(np.array(train_data1), chunks=(1, data_sizeX, data_sizeY, num_channel))\n",
    "train_data2 = da.from_array(np.array(train_data2), chunks=(1, data_sizeX, data_sizeY, num_channel))\n",
    "train_mask = da.from_array(np.array(train_mask), chunks=(1, mask_sizeX, mask_sizeY, 1))\n",
    "test_data1 = da.from_array(np.array(test_data1), chunks=(1, data_sizeX, data_sizeY, num_channel))\n",
    "test_data2 = da.from_array(np.array(test_data2), chunks=(1, data_sizeX, data_sizeY, num_channel))\n",
    "test_mask = da.from_array(np.array(test_mask), chunks=(1, mask_sizeX, mask_sizeY, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_model():\n",
    "    inputs1 = keras.layers.Input(shape=(data_sizeX, data_sizeY, num_channel))\n",
    "    inputs2 = keras.layers.Input(shape=(data_sizeX, data_sizeY, num_channel))\n",
    "\n",
    "    # Diffusion process\n",
    "    diff1 = keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu')(inputs1)\n",
    "    diff1 = keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu')(diff1)\n",
    "    diff2 = keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu')(inputs2)\n",
    "    diff2 = keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu')(diff2)\n",
    "\n",
    "    # Fusion\n",
    "    fuse = keras.layers.Add()([diff1, diff2])\n",
    "    outputs = keras.layers.Conv2D(1, (3, 3), padding='same', activation='sigmoid')(fuse)\n",
    "\n",
    "    model = keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/4\n",
      "1875/1875 [==============================] - 4118s 2s/step - loss: 1113.6929\n",
      "Epoch 2/4\n",
      "1875/1875 [==============================] - 4297s 2s/step - loss: 1113.6616\n",
      "Epoch 3/4\n",
      "1875/1875 [==============================] - 5393s 3s/step - loss: 1113.6534\n",
      "Epoch 4/4\n",
      "1875/1875 [==============================] - 3774s 2s/step - loss: 1113.6456\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "47/47 [==============================] - 81s 2s/step - loss: 1113.6819\n",
      "Test Loss: 1113.681884765625\n"
     ]
    }
   ],
   "source": [
    "model = diffusion_model()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "model.fit([train_data1, train_data2], train_mask, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "\n",
    "test_loss = model.evaluate([test_data1, test_data2], test_mask)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = path + sub_path\n",
    "model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "47/47 [==============================] - 83s 2s/step\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict([test_data1, test_data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1_np = test_data1.compute()\n",
    "test_data2_np = test_data2.compute()\n",
    "test_mask_np = test_mask.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat(path + sub_path + 'test_data_dask.mat', {'test_data1': test_data1_np})\n",
    "sio.savemat(path + sub_path + 'test_data_dask.mat', {'test_data2': test_data2_np})\n",
    "sio.savemat(path + sub_path + 'test_mask_dask.mat', {'test_mask': test_mask_np})\n",
    "sio.savemat(path + sub_path + 'test_pred_dask.mat', {'test_pred': test_pred})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
